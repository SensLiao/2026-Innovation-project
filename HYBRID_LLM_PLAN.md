# SOMA Hybrid Local LLM Architecture Plan

## Branch: `feature/Steven-hybrid-local-llm`

## Overview

This branch implements a **hybrid LLM architecture** that combines:
- **Local Models** (Ollama/MLX) for privacy-sensitive medical analysis
- **Cloud APIs** (Claude/OpenAI) for complex reasoning tasks
- **Intelligent Routing** to optimize cost, latency, and privacy

## Hardware Requirements

Tested on:
- **Mac Studio 2025** - Apple M4 Max, 36GB RAM
- Capable of running 8B-32B parameter models efficiently

## Architecture Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       LLM Provider Layer                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚  Local Provider â”‚    â”‚  Cloud Provider â”‚                   â”‚
â”‚   â”‚                 â”‚    â”‚                 â”‚                   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚
â”‚   â”‚  â”‚  Ollama   â”‚  â”‚    â”‚  â”‚  Claude   â”‚  â”‚                   â”‚
â”‚   â”‚  â”‚  - Llama  â”‚  â”‚    â”‚  â”‚  Sonnet   â”‚  â”‚                   â”‚
â”‚   â”‚  â”‚  - Qwen   â”‚  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚
â”‚   â”‚  â”‚  - Mistralâ”‚  â”‚    â”‚                 â”‚                   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚
â”‚   â”‚                 â”‚    â”‚  â”‚  OpenAI   â”‚  â”‚                   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”‚  GPT-4o   â”‚  â”‚                   â”‚
â”‚   â”‚  â”‚    MLX    â”‚  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚
â”‚   â”‚  â”‚ (Apple)   â”‚  â”‚    â”‚                 â”‚                   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚                 â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                 â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                    â”‚  Smart Router   â”‚                         â”‚
â”‚                    â”‚  - Task Type    â”‚                         â”‚
â”‚                    â”‚  - Complexity   â”‚                         â”‚
â”‚                    â”‚  - Privacy Req  â”‚                         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Agent Layer                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Radiologist  â”‚  â”‚ Pathologist  â”‚  â”‚ ReportWriter â”‚          â”‚
â”‚  â”‚    Agent     â”‚  â”‚    Agent     â”‚  â”‚    Agent     â”‚          â”‚
â”‚  â”‚  (Local)     â”‚  â”‚  (Local)     â”‚  â”‚  (Cloud)     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚  â”‚  QCReviewer  â”‚  â”‚  Alignment   â”‚                            â”‚
â”‚  â”‚    Agent     â”‚  â”‚    Agent     â”‚                            â”‚
â”‚  â”‚  (Local)     â”‚  â”‚  (Hybrid)    â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Recommended Model Assignments

| Agent | Task Type | Recommended Model | Fallback |
|-------|-----------|-------------------|----------|
| **RadiologistAgent** | Image Analysis | Llama 3.2 Vision 11B (Local) | Claude Sonnet |
| **PathologistAgent** | Diagnosis | Qwen 2.5 32B (Local) | Claude Sonnet |
| **ReportWriterAgent** | Report Generation | Claude Sonnet (Cloud) | Llama 3.2 70B |
| **QCReviewerAgent** | Quality Check | Llama 3.2 8B (Local) | - |
| **AlignmentAgent** | Intent Classification | Llama 3.2 3B (Local) | - |

### Why This Assignment?

1. **RadiologistAgent (Local)**: Medical image analysis needs privacy, local vision models work well
2. **PathologistAgent (Local)**: Diagnosis data is sensitive, can use specialized medical models
3. **ReportWriterAgent (Cloud)**: Needs best language quality for professional reports
4. **QCReviewer (Local)**: Simple checklist validation, lightweight model sufficient
5. **AlignmentAgent (Local)**: Fast intent classification, low latency needed

## Model Options for M4 Max 36GB

### Recommended Local Models

```bash
# Fast & Efficient (8B params, ~5GB VRAM)
ollama pull llama3.2:8b
ollama pull qwen2.5:7b

# Balanced (14B-32B params, ~20GB VRAM)
ollama pull qwen2.5:32b
ollama pull llama3.2:14b

# Vision Capable
ollama pull llama3.2-vision:11b

# Medical Specialized (if available)
ollama pull meditron:7b
ollama pull biomistral:7b
```

### MLX Models (Apple Silicon Optimized)

```python
# Using mlx-lm library
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Llama-3.2-3B-Instruct-4bit")
```

## Implementation Plan

### Phase 1: Provider Abstraction Layer

Create `backend/providers/` directory:

```
backend/providers/
â”œâ”€â”€ index.js              # Provider factory
â”œâ”€â”€ baseProvider.js       # Abstract provider interface
â”œâ”€â”€ claudeProvider.js     # Anthropic Claude API
â”œâ”€â”€ openaiProvider.js     # OpenAI API
â”œâ”€â”€ ollamaProvider.js     # Ollama local models
â”œâ”€â”€ mlxProvider.js        # MLX Python bridge (optional)
â””â”€â”€ router.js             # Intelligent routing logic
```

### Phase 2: Update BaseAgent

Modify `backend/agents/baseAgent.js`:
- Add provider selection logic
- Support multiple LLM backends
- Fallback mechanism

### Phase 3: Smart Router

Implement routing based on:
1. **Task Complexity**: Simple â†’ Local, Complex â†’ Cloud
2. **Privacy Level**: High â†’ Local, Low â†’ Cloud
3. **Latency Requirements**: Real-time â†’ Local, Batch â†’ Cloud
4. **Cost Optimization**: Prefer local when quality is acceptable

### Phase 4: Configuration

Create `backend/config/llm.config.js`:

```javascript
export default {
  providers: {
    ollama: {
      enabled: true,
      baseUrl: 'http://localhost:11434',
      models: {
        fast: 'llama3.2:3b',
        balanced: 'qwen2.5:14b',
        vision: 'llama3.2-vision:11b'
      }
    },
    claude: {
      enabled: true,
      model: 'claude-sonnet-4-20250514'
    },
    openai: {
      enabled: false,
      model: 'gpt-4o-mini'
    }
  },

  routing: {
    // Agent â†’ Provider mapping
    radiologist: { primary: 'ollama:vision', fallback: 'claude' },
    pathologist: { primary: 'ollama:balanced', fallback: 'claude' },
    reportWriter: { primary: 'claude', fallback: 'ollama:balanced' },
    qcReviewer: { primary: 'ollama:fast', fallback: null },
    alignment: { primary: 'ollama:fast', fallback: null }
  },

  // Auto-fallback on error
  autoFallback: true,

  // Latency threshold for fallback (ms)
  latencyThreshold: 30000
}
```

## Files to Create/Modify

### New Files

| File | Description |
|------|-------------|
| `backend/providers/index.js` | Provider factory & exports |
| `backend/providers/baseProvider.js` | Abstract provider interface |
| `backend/providers/claudeProvider.js` | Claude API wrapper |
| `backend/providers/ollamaProvider.js` | Ollama API wrapper |
| `backend/providers/router.js` | Smart routing logic |
| `backend/config/llm.config.js` | LLM configuration |
| `backend/scripts/setup-ollama.sh` | Ollama setup script |

### Modified Files

| File | Changes |
|------|---------|
| `backend/agents/baseAgent.js` | Use provider abstraction |
| `backend/agents/*.js` | Add provider preference hints |
| `backend/.env` | Add Ollama config vars |
| `backend/package.json` | Add ollama dependencies |

## API Compatibility

The hybrid version maintains **full API compatibility** with the existing frontend:
- Same `/medical_report_init` endpoint
- Same `/chat_stream` SSE format
- Same response structures

Frontend changes: **NONE REQUIRED**

## Environment Variables

Add to `backend/.env`:

```bash
# LLM Provider Settings
LLM_PRIMARY_PROVIDER=ollama          # ollama | claude | openai
LLM_FALLBACK_PROVIDER=claude         # Fallback when primary fails

# Ollama Settings
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_FAST=llama3.2:3b
OLLAMA_MODEL_BALANCED=qwen2.5:14b
OLLAMA_MODEL_VISION=llama3.2-vision:11b

# Cloud API Keys (existing)
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...

# Routing Settings
AUTO_FALLBACK=true
LATENCY_THRESHOLD_MS=30000
```

## Testing Plan

1. **Unit Tests**: Each provider independently
2. **Integration Tests**: Provider switching & fallback
3. **E2E Tests**: Full analysis flow with local models
4. **Performance Benchmarks**: Compare local vs cloud latency

## Development Order

1. [ ] Create provider abstraction layer
2. [ ] Implement OllamaProvider
3. [ ] Update BaseAgent to use providers
4. [ ] Implement smart router
5. [ ] Add configuration system
6. [ ] Test with each agent
7. [ ] Add fallback logic
8. [ ] Performance tuning
9. [ ] Documentation

## Actual Benchmark Results (Mac Studio M4 Max, 36GB)

**æµ‹è¯•æ—¥æœŸ**: 2025-11-29
**æ¨¡å‹**: Qwen3 32B (Q4_K_M quantization, ~20GB)

### å•ä»»åŠ¡å»¶è¿Ÿ

| Agent Task | å»¶è¿Ÿ | Tokens | è´¨é‡ | æ¨è |
|------------|------|--------|------|------|
| Intent Classification | 12.4s | 247 | âœ… æ­£ç¡® | ğŸ  Local |
| Radiologist Analysis | 19.6s | ~350 | âœ… ä¸“ä¸š | ğŸ  Local |
| Pathologist Diagnosis | 49.6s | 800 | âœ… è¯¦ç»† | ğŸ  Local |
| Report Writer | 28.3s | 451 | âœ… é«˜è´¨é‡ | âš ï¸ Hybrid |
| QC Reviewer | 14.5s | 229 | âœ… å‡†ç¡® | ğŸ  Local |

### å®Œæ•´ Pipeline (Sequential)

```
Stage 1: Intent Classification  â†’ 12.4s
Stage 2: Radiologist Analysis   â†’ 24.6s
Stage 3: Pathologist Diagnosis  â†’ 24.9s
Stage 4: Report Writer          â†’ 21.7s
Stage 5: QC Review              â†’ 12.5s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL PIPELINE TIME             â†’ 96.2s (~1.6 min)
```

### ä¼˜åŒ–åä¼°è®¡ (Parallel Execution)

```
AlignmentAgent (12s)
      â”‚
      â”œâ”€ [PARALLEL] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   RadiologistAgent (25s)    â”‚
      â”‚   PathologistAgent (25s) â”€â”€â”€â”¤ max: 25s
      â”‚                             â”‚
      â”œâ”€ ReportWriterAgent (22s)    â”‚
      â””â”€ QCReviewerAgent (12s)

Optimized Total: ~71s (26% faster)
```

### æˆæœ¬å¯¹æ¯”

| æ–¹æ¡ˆ | å•æ¬¡åˆ†æ | 1000æ¬¡/æœˆ | åŸºç¡€è®¾æ–½ |
|------|----------|-----------|----------|
| Cloud Only (Claude) | $0.08 | $80 | æ—  |
| Hybrid | $0.02 | $20 | Mac Studio |
| Full Local | $0.002 | $2 (ç”µè´¹) | Mac Studio |

### Qwen3 Thinking Mode è¯´æ˜

Qwen3 é»˜è®¤å¯ç”¨ "thinking" æ¨¡å¼ï¼š
- è¾“å‡ºå‰ä¼šç”Ÿæˆ ~200-300 tokens çš„å†…éƒ¨æ¨ç†
- æé«˜è´¨é‡ä½†å¢åŠ å»¶è¿Ÿ
- ä»£ç ä¸­å·²æ·»åŠ  `thinkingBudget` è¡¥å¿

## Rollback Plan

If issues arise:
1. Set `LLM_PRIMARY_PROVIDER=claude` to use cloud-only
2. All agents fall back to original Claude implementation
3. No frontend changes needed

---

**Author**: Claude Code + Steven
**Created**: 2025-11-29
**Branch**: `feature/Steven-hybrid-local-llm`
