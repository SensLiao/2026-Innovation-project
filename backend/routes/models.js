
import express from 'express';
import multer from 'multer';
import sharp from 'sharp';
import * as ort from 'onnxruntime-node';
import fs from 'fs';
import axios from 'axios';

import globals from '../globals.js';

const router = express.Router();

// 2. Variables initialization
const storage = multer.memoryStorage();
const upload = multer({ storage });

// 3. Load Image as tensor
async function loadImageAsTensor(imageBuffer) {
  // Step 1: ‰ΩøÁî® sharp ËØªÂèñ BufferÔºåËé∑ÂèñÂéüÂßãÂÉèÁ¥†Êï∞ÊçÆÔºàHWC, Uint8Ôºâ
  const { data, info } = await sharp(imageBuffer).raw().toBuffer({ resolveWithObject: true });

  console.log('Channels:', info.channels);
  const height = info.height;
  const width = info.width;

  // Step 2: ËΩ¨Êàê float32Ôºå‰øùÁïô [0, 255] ÂÄºÂüü
  const floatData = Float32Array.from(data);

  // Step 3: ÊûÑÈÄ† ONNX ËæìÂÖ• Tensor (H, W, 3)
  const tensor = new ort.Tensor('float32', floatData, [height, width, 3]);

  console.log('‚úÖ ÊµãËØïÂõæÁâáËΩ¨Êç¢‰∏∫ Tensor ÊàêÂäüÔºÅ');
  return tensor;
}

// 4. image encoder running
async function runImageEncoder(imageTensor, encoder) {
  // Step 1: ÊâßË°åÊé®ÁêÜ
  const feeds = { input_image: imageTensor };
  const results = await encoder.run(feeds);

  // Step 2: Ëé∑ÂèñËæìÂá∫ÁªìÊûú
  const embedding = results.image_embeddings;

  // Step3: ÊâìÂç∞ËæìÂá∫ÁªìÊûú
  console.log('‚úÖ image encoder Êé®ÁêÜÊàêÂäüÔºÅ');
  console.log('ËæìÂá∫Áª¥Â∫¶:', embedding.dims);

  return embedding; // ËøîÂõûËæìÂá∫ÁªìÊûú
}

// 5. Image decoder running
async function runImageDecoder(feeds, decoder) {
  const results = await decoder.run(feeds);
  // results : {"masks", "iou_predictions", "low_res_masks"}
  const masks = results.masks;
  const iou_predictions = results.iou_predictions;
  const low_res_masks = results.low_res_masks;
  console.log('‚úÖ image decoder Êé®ÁêÜÊàêÂäüÔºÅ');

  return { masks, iou_predictions, low_res_masks };
}

// 6. Convert normalArray to float32Array
function convertToTensor(normalArray, dims) {
  const float32Array = new Float32Array(normalArray);
  const tensor = new ort.Tensor('float32', float32Array, dims);
  return tensor;
}

// 7.Image encoder route
router.post('/load_model', upload.single('image'), async (req, res) => {
  try {
    //1. get the image file from the frontend
    const file = req.file;
    if (!file) {
      return res.status(400).json({ error: 'No image file uploaded' });
    } else {
      console.log('‚úÖ Êî∂Âà∞ÂõæÁâá:', file.originalname, file.mimetype, file.size);
    }

    //2.load the ONNX image encoder model
    const encoder = globals.onnxModels.encoder;

    //3. convert the image to tensor
    const imageTensor = await loadImageAsTensor(file.buffer);
    console.log('‚úÖ ÂõæÁâáËΩ¨Êç¢‰∏∫ Tensor ÊàêÂäüÔºÅ');

    //4. run the image encoder
    const image_embeddings = await runImageEncoder(imageTensor, encoder);
    console.log('‚úÖ ÂõæÁâáembedding ÊàêÂäüÔºÅ');
    return res.status(200).json({
      message: 'Image embeddings generated successfully',
      image_embeddings: Array.from(image_embeddings.data),
      embedding_dims: image_embeddings.dims
    });
  } catch (error) {
    console.error('‚ùå Error loading model:', error);
    return res.status(500).json({ error: 'Failed to output the image embedding' });
  }
});

// 8.Image decoder route
router.post('/run_model', async (req, res) => {
  try {
    let {
      image_embeddings,   // float[] (flatten)
      embedding_dims,     // e.g. [1, 256, 64, 64]
      point_coords,       // e.g. [[x,y], ...] (ÂÉèÁ¥†ÂùêÊ†á)
      point_labels,       // e.g. [1, 0, ...]
      mask_input,         // optional: float[] (1*1*lowRes*lowRes)
      has_mask_input,     // optional: [0] or [1]
      orig_im_size        // [H, W]
    } = req.body;

    if (!image_embeddings || !embedding_dims) {
      return res.status(400).json({ error: 'Image embeddings not generated yet' });
    }

    const decoder = globals.onnxModels.decoder;
    if (!decoder) {
      return res.status(500).json({ error: 'Image decoder model not loaded' });
    }

    // ---- helpersÔºàÂè™Âú®Ê≠§Ë∑ØÁî±ÂÜÖÁî®Ôºâ----
    const getTargetLength = (embeddingDims) => {
      // SAM/MedSAMÔºögrid * 16
      const grid = embeddingDims[2];
      return grid * 16;
    };

    const applyCoordsToTarget = (points, origH, origW, targetLength) => {
      const scale = targetLength / Math.max(origH, origW);
      const newH = Math.round(origH * scale);
      const newW = Math.round(origW * scale);
      return points.map(([x, y]) => [
        x * (newW / origW),
        y * (newH / origH)
      ]);
    };

    const logitsTo01Flat = (arr) => {
      const out = new Uint8Array(arr.length);
      for (let i = 0; i < arr.length; i++) out[i] = arr[i] > 0 ? 1 : 0;
      return out;
    };

    // 0) ÂéüÂõæÂ∞∫ÂØ∏
    let [origH, origW] = Array.isArray(orig_im_size) ? orig_im_size : [0, 0];
    origH = Number(origH);
    origW = Number(origW);

    // 1) Áî± embedding_dims Êé®Âõû targetLength / lowRes
    const targetLength = getTargetLength(embedding_dims); // Â∏∏ËßÅ 1024 Êàñ 256
    const lowRes = targetLength / 4;                      // Â∏∏ËßÅ 256 Êàñ 64

    console.log('‚úÖ embedding_dims:', embedding_dims, '‚Üí targetLength:', targetLength, 'lowRes:', lowRes);

    // 2) image_embeddings
    const embTensor = new ort.Tensor('float32', Float32Array.from(image_embeddings), embedding_dims);

    // 3) points + labelsÔºöÂùêÊ†áÊò†Â∞Ñ + padding ÁÇπ
    const pts = Array.isArray(point_coords) ? point_coords : [];
    const lbs = Array.isArray(point_labels) ? point_labels : [];

    const mapped = applyCoordsToTarget(pts, origH, origW, targetLength);
    // ËøΩÂä† padding ÁÇπ (0,0), label = -1ÔºàÊó† box Êó∂Âª∫ËÆÆÊÄªË°•Ôºâ
    mapped.push([0.0, 0.0]);
    lbs.push(-1);

    const numPts = mapped.length;
    const coordsTensor = new ort.Tensor('float32', Float32Array.from(mapped.flat()), [1, numPts, 2]);
    const labelsTensor = new ort.Tensor('float32', Float32Array.from(lbs), [1, numPts]);

    // 4) mask_input / has_mask_inputÔºöÂ∞∫ÂØ∏Âä®ÊÄÅÂåπÈÖç lowRes
    const expectedLen = 1 * 1 * lowRes * lowRes;
    let maskTensor, hasMaskTensor;

    if (mask_input && mask_input.length === expectedLen) {
      maskTensor = new ort.Tensor('float32', Float32Array.from(mask_input), [1, 1, lowRes, lowRes]);
      if (has_mask_input && has_mask_input.length === 1) {
        hasMaskTensor = new ort.Tensor('float32', Float32Array.from(has_mask_input), [1]);
      } else {
        const anyNonZero = mask_input.some((v) => v !== 0);
        hasMaskTensor = new ort.Tensor('float32', Float32Array.from([anyNonZero ? 1 : 0]), [1]);
      }
    } else {
      maskTensor = new ort.Tensor('float32', new Float32Array(expectedLen), [1, 1, lowRes, lowRes]); // ÂÖ®Èõ∂
      hasMaskTensor = new ort.Tensor('float32', Float32Array.from([0]), [1]);
    }

    // 5) orig_im_size
    const sizeTensor = new ort.Tensor('float32', Float32Array.from([origH, origW]), [2]);

    const feeds = {
      image_embeddings: embTensor,
      point_coords: coordsTensor,
      point_labels: labelsTensor,
      mask_input: maskTensor,
      has_mask_input: hasMaskTensor,
      orig_im_size: sizeTensor
    };
    const { masks, iou_predictions, low_res_masks } = await runImageDecoder(feeds, decoder);

    console.log('Mask shape:', masks.dims); // ÂÖ∏Âûã [1,1,H,W]
    console.log('Low-res masks shape:', low_res_masks.dims); // ÂÖ∏Âûã [1,1,lowRes,lowRes]
    console.log('IOU predictions:', Array.from(iou_predictions.data));

    // 6) Áõ¥Êé• logits > 0
    const final_mask = Array.from(logitsTo01Flat(Array.from(masks.data)));

    return res.status(200).json({
      message: 'Image decoder ran successfully',
      masks: final_mask,
      masks_shape: masks.dims,
      iou_predictions: Array.from(iou_predictions.data),
      iou_shape: iou_predictions.dims,
      low_res_masks: Array.from(low_res_masks.data),
      low_res_masks_shape: low_res_masks.dims
    });
  } catch (error) {
    console.error('‚ùå Error running model:', error);
    return res.status(500).json({ error: 'Failed to run the image decoder' });
  }
});

// 9. Medical Analysis router -- need to be implemented
router.post('/medical_report_init', async (req, res) => {
  try {
    const { final_image } = req.body;

    if (!final_image) {
      return res.status(400).json({ error: 'No final image provided' });
    }

    // 1. prepare the json prompt message for report generation
    const systemPrompt = `
    # Role
    You are a board-certified radiologist and medical imaging expert.

    # Inputs
    You will receive ONE overlay image (original image + lesion mask in blue). Pixel spacing may be unknown.

    # Task
    - Analyze ONLY the provided image.
    - Describe objective findings (location, shape/margins, density/signal, relation to adjacent structures, pertinent negatives).
    - Provide a concise impression with differentials ranked by likelihood and short justifications.
    - Be conservative; clearly separate findings vs diagnostic impression.

    # Hard Rules (Critical for automation)
    1) Return **ONLY** a single valid JSON object. **No code fences, no prose, no Markdown, no trailing commas.**
    2) Use **exact keys and types** as specified below. **Do not add extra keys. Do not rename keys.**
    3) If a value is unknown, use an empty string '' or an empty array '[]' (avoid 'null').
    4) Probabilities are **0.0-1.0**. Arrays may be empty but must exist where required.

    # Additional guidance
    - Keep text concise and clinically useful with inference detail.
    - If spacing is unknown, keep sizes in pixels and state that limitation in disclaimers.`;

    const userMessage_final = `
    {
      "images": {
        "overlay": "<base64 or URL>",
        "original": "<base64 or URL (optional)>"
      },
      "language": "Chinese",
      "study_info": {
        "modality": "<CXR | CT | MRI | US | Pathology ‚Ä¶>",
        "body_part": "<chest | brain | abdomen | pelvis | bone | ‚Ä¶>",
        "patient": {
          "sex": "<M | F | Unknown>",
          "age": "<number + unit, e.g., 63y>"
        },
        "acquisition_notes": "<CT-portal venous phase / MRI T2WI / DWI b=800, etc. (optional)>",
        "pixel_spacing": "<e.g., 0.7x0.7 mm or 'N/A' if unknown (optional)>"
      },
      "roi_stats_px": {
        "area_px": "<integer>",
        "long_diameter_px": "<integer>",
        "short_diameter_px": "<integer>",
        "mean_intensity": "<float>",
        "std_intensity": "<float>",
        "multiplicity_n": "<integer if multiple; omit if single (optional)>"
      },
      "notes": "No physical spacing; all dimensions in pixels; blue is the ROI overlay."
    }`;

    const userMessage_test = `
    {
      "user message": "Please analyze the lesion area in this medical image and generate a report.",
      "images": {
        "overlay_image": ${JSON.stringify(final_image)}
      }
    }`;

    const message = {
      'user message': userMessage_test,
      'system prompt': systemPrompt,
      temperature: 0.1,
      Language: 'Chinese'
    };

    // 2. Prepare the request to the medical report generation API
    const url = 'http://localhost:5678/webhook-test/15f56758-4d20-48e2-aca8-13188bf401d7'; // n8n webhook URL

    const response = await axios.post(url, message, {
      timeout: 120000, // Ë∂ÖÊó∂ÊØ´Áßí
      headers: { 'Content-Type': 'application/json' }
    });

    // 3. Send the response back to the frontend
    const payloadOut = response.data?.data ?? response.data;
    console.log('‚úÖ Medical report generated successfully');
    fs.writeFileSync('report.json', JSON.stringify(response.data, null, 2)); // ‰øùÂ≠òÊä•ÂëäÂà∞Êñá‰ª∂
    return res.status(200).json({ message: 'Medical report generated successfully', report: payloadOut });
  } catch (error) {
    console.error('‚ùå Error generating report:', error);
    try {
      const data = fs.readFileSync('report.json', 'utf8');
      const json_data = JSON.parse(data);
      const payloadOut = json_data?.data?.output ?? json_data;
      return res.status(200).json({ message: 'Medical report generated successfully', report: payloadOut });
    } catch {
      return res.status(500).json({ error: 'Failed to generate the medical report' });
    }
  }
});

// 10. Report generation router -- need to be implemented
router.post('/medical_report_rein', async (req, res) => {
  console.log('üöß This feature is not implemented yet');
  res.status(501).json({ error: 'This feature is not implemented yet' });
});

// 11.Testing functions
async function test(fileBuffer) {
  // --- tiny helpers (only for this test) ---
  const getTargetLength = (embeddingDims) => {
    // embeddingDims ÂΩ¢Â¶Ç [1, 256, G, G]ÔºåG*16 Â∞±ÊòØÈïøËæπ target_length
    const grid = embeddingDims[2];
    return grid * 16;
  };

  const applyCoordsToTarget = (points, origH, origW, targetLength) => {
    const scale = targetLength / Math.max(origH, origW);
    const newH = Math.round(origH * scale);
    const newW = Math.round(origW * scale);
    return points.map(([x, y]) => [x * (newW / origW), y * (newH / origH)]);
  };

  const logitsTo01Flat = (flat) => {
    const out = new Uint8Array(flat.length);
    for (let i = 0; i < flat.length; i++) out[i] = flat[i] > 0 ? 1 : 0;
    return out;
  };

  const to2D = (flat, h, w) => {
    const rows = [];
    for (let y = 0; y < h; y++) {
      rows.push(Array.from(flat.slice(y * w, (y + 1) * w)));
    }
    return rows;
  };

  // 1) ÂéüÂõæÂ∞∫ÂØ∏
  const { width: origW, height: origH } = await sharp(fileBuffer).metadata();

  // 2) ÁºñÁ†ÅÂô®ÔºöÊ≥®ÊÑè‰Ω†Â∑≤Áî® --use-preprocessÔºåÊâÄ‰ª•Áõ¥Êé•ÂñÇ HWC/0-255 Âç≥ÂèØ
  const imageTensor = await loadImageAsTensor(fileBuffer);
  const embedding = await runImageEncoder(imageTensor, globals.onnxModels.encoder);

  // 3) Áî± embedding Áª¥Â∫¶Êé®Âõû target_length & low-res Â∞∫ÂØ∏
  const targetLength = (function getTL(dims) {
    // dims e.g. [1, 256, 64, 64] => targetLength = 64 * 16 = 1024
    const grid = dims[2];
    return grid * 16;
  })(embedding.dims);
  const lowRes = targetLength / 4;

  console.log('üîé embedding.dims =', embedding.dims, '=> targetLength =', targetLength, 'lowRes =', lowRes);

  // 4) ÊûÑÈÄ†ÊèêÁ§∫ÔºöÂçïÁÇπ + padding ÁÇπ
  const rawPoints = [[336, 275]];
  const rawLabels = [1]; // 1=Ê≠£ÁÇπ
  const mapped = applyCoordsToTarget(rawPoints, origH, origW, targetLength);

  // ËøΩÂä† padding ÁÇπ (0,0), label=-1 ‰ª•Á¨¶ÂêàËß£Á†ÅÂô®Á∫¶ÂÆö
  mapped.push([0.0, 0.0]);
  rawLabels.push(-1);

  const numPts = mapped.length;
  const coordsTensor = new ort.Tensor('float32', Float32Array.from(mapped.flat()), [1, numPts, 2]);
  const labelsTensor = new ort.Tensor('float32', Float32Array.from(rawLabels), [1, numPts]);

  // 5) ÂáÜÂ§á mask_inputÔºàÂÖ®Èõ∂Ôºâ„ÄÅhas_mask_inputÔºà0Ôºâ
  const mask_input = new ort.Tensor('float32', new Float32Array(1 * 1 * lowRes * lowRes), [1, 1, lowRes, lowRes]);
  const has_mask_input = new ort.Tensor('float32', Float32Array.from([0]), [1]);

  // 6) orig_im_size
  const orig_im_size = new ort.Tensor('float32', Float32Array.from([origH, origW]), [2]);

  // 7) Ëß£Á†ÅÂô®
  const decoder = globals.onnxModels.decoder;
  const feeds = {
    image_embeddings: embedding,
    point_coords: coordsTensor,
    point_labels: labelsTensor,
    mask_input: mask_input,
    has_mask_input: has_mask_input,
    orig_im_size: orig_im_size
  };

  const { masks, iou_predictions, low_res_masks } = await runImageDecoder(feeds, decoder);

  console.log('‚úÖ decoder done. masks shape:', masks.dims, 'low_res_masks shape:', low_res_masks.dims);
  console.log('IOU predictions:', Array.from(iou_predictions.data));

  // 8) logits Áõ¥Êé• > 0 ÂæóÂà∞ 0/1
  const masksFlat01 = logitsTo01Flat(Array.from(masks.data));

  // 9) Áî®ÁúüÂÆûÁª¥Â∫¶ÂÜôÂÖ•Ë∞ÉËØïÊñá‰ª∂Ôºà‰∏çË¶ÅÁ°¨ÁºñÁ†Å 512x512Ôºâ
  const H = masks.dims[2],
    W = masks.dims[3];
  const masks2D = to2D(masksFlat01, H, W);
  fs.writeFileSync('mask.txt', masks2D.map((row) => row.join(' ')).join('\n'));

  console.log('üìù mask written to mask.txt with shape', H, 'x', W);
}

// ESM ÂØºÂá∫ÔºàÂëΩÂêçÂØºÂá∫Ôºâ
export { router, test };
